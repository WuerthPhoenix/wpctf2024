{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ccf7febf769b261",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training classifier for our cats vs dogs v2 challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5a13508ef65ea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d26d685b2bb50913",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T10:23:43.258887Z",
     "start_time": "2024-10-18T10:23:43.254686Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import tensorflow as tf\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc15c02",
   "metadata": {},
   "source": [
    "Download the dataset as a zip file and extract it to the target directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1b61724156e6509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T10:24:12.470067Z",
     "start_time": "2024-10-18T10:23:45.042332Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "dataset_url = \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\"\n",
    "\n",
    "\n",
    "r = requests.get(dataset_url, stream=True)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall(\"./dataset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89387b909389f7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Read input images using keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2504ee35058f4c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we remove some images that are corrupted, so that cannot be correctly loaded. We simply try to open them using the same method that will be applied by the Keras helper we use to create the dataset and then we remove those that return us an exception"
   ]
  },
  {
   "cell_type": "code",
   "id": "59d7a0cc1dc3f1e2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "image_dataset_path = \"./dataset/PetImages\"\n",
    "dogs_images_dir = f\"{image_dataset_path}/Dog/\"\n",
    "cats_images_dir = f\"{image_dataset_path}/Cat/\"\n",
    "\n",
    "def remove_corrupted_images(dir_path):\n",
    "    images = os.listdir(dir_path)\n",
    "    \n",
    "    for image in images:\n",
    "        image_full_path = f\"{dir_path}/{image}\"\n",
    "        try:\n",
    "            img_bytes = tf.io.read_file(image_full_path)\n",
    "            decoded_img = tf.io.decode_image(img_bytes)\n",
    "            if len(decoded_img.shape) != 3:\n",
    "                print(f\"Removing image {image_full_path}\")\n",
    "                os.remove(image_full_path)\n",
    "        except tf.errors.InvalidArgumentError as e:\n",
    "            print(f\"Removing image {image_full_path}\")\n",
    "            os.remove(image_full_path)\n",
    "        \n",
    "remove_corrupted_images(dogs_images_dir)\n",
    "remove_corrupted_images(cats_images_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "48fc4689",
   "metadata": {},
   "source": [
    "Define the image size and the batch size used during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccceaedce6dda20c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T10:26:26.947796Z",
     "start_time": "2024-10-18T10:26:26.944657Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# general training settings\n",
    "img_height = img_width = 64\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adebd97ce7873dbe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then we create the training set and validation set, according to a 80/20 proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cfd7c76e8b92c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T10:26:28.867405Z",
     "start_time": "2024-10-18T10:26:28.244693Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24940 files belonging to 2 classes.\n",
      "Using 19952 files for training.\n",
      "Using 4988 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    image_dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    label_mode=\"categorical\",\n",
    "    seed=123,\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2f32c848d8183d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T10:26:31.010796Z",
     "start_time": "2024-10-18T10:26:31.007151Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cat', 'Dog']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b165e98013f0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-21T17:41:25.723955336Z",
     "start_time": "2023-10-21T17:41:25.666878153Z"
    },
    "collapsed": false
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc1a8f9899eab1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We use a quite simple architecture that we can somehow divide as follows: \n",
    "\n",
    "- Rescaling the images to ensure the value of the pixel is between 0 and 1, to help the training of the model\n",
    "- A couple of convolutional layers to capture the geometrical characteristics of the images\n",
    "- Dense layers to then bring the results towards the output layer that we desire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85f3e5452cc02682",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T10:26:33.356220Z",
     "start_time": "2024-10-18T10:26:33.238140Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(img_height, img_width, 3)))\n",
    "model.add(tf.keras.layers.Rescaling(1./255))\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c364832c6ada590d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T10:26:34.540772Z",
     "start_time": "2024-10-18T10:26:34.519113Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18432</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling (\u001B[38;5;33mRescaling\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m3\u001B[0m)      │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001B[38;5;33mConv2D\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m62\u001B[0m, \u001B[38;5;34m62\u001B[0m, \u001B[38;5;34m32\u001B[0m)     │           \u001B[38;5;34m896\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001B[38;5;33mMaxPooling2D\u001B[0m)    │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m31\u001B[0m, \u001B[38;5;34m31\u001B[0m, \u001B[38;5;34m32\u001B[0m)     │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m29\u001B[0m, \u001B[38;5;34m29\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │        \u001B[38;5;34m18,496\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m12\u001B[0m, \u001B[38;5;34m12\u001B[0m, \u001B[38;5;34m128\u001B[0m)    │        \u001B[38;5;34m73,856\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001B[38;5;33mFlatten\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m18432\u001B[0m)          │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │     \u001B[38;5;34m1,179,712\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001B[38;5;33mDropout\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)             │         \u001B[38;5;34m2,080\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m16\u001B[0m)             │           \u001B[38;5;34m528\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2\u001B[0m)              │            \u001B[38;5;34m34\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,275,602</span> (4.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m1,275,602\u001B[0m (4.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,275,602</span> (4.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m1,275,602\u001B[0m (4.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45efbea1e557443",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's start with a small learning rate, using the Adam optimizer and the categorical cross entropy as a loss measure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99ee37d6928312d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T10:26:36.681183Z",
     "start_time": "2024-10-18T10:26:36.669241Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea06301202cba28",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Time to train!"
   ]
  },
  {
   "cell_type": "code",
   "id": "b963f4be4a11536c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "history = model.fit(train_ds, validation_data=val_ds, epochs = 12)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bcfffda9ad4d9a28",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Saving the model to be used in the challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd627660cf0bfc42",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Final notes: yes, the training part is missing the following aspects:\n",
    "\n",
    "  - usage of a test set, usually a best practice\n",
    "  - some shuffling of the input data\n",
    "  - and since we are talking about images, some basic transformations such as zooming, rotations, etc to allow a better generalization \n",
    "\n",
    " but hey, our goal was to have a model good enough for the challenge!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd9baf4",
   "metadata": {},
   "source": [
    "We then convert the model to tflite, to allow us to just load the inference part"
   ]
  },
  {
   "cell_type": "code",
   "id": "e13cf1c2b66450d9",
   "metadata": {},
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0adae51",
   "metadata": {},
   "source": [
    "Save the model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a87fcdcd20fdf665",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T10:30:15.341344Z",
     "start_time": "2024-10-18T10:30:15.334880Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
